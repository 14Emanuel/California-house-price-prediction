{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nThis project will be solving a supervised Machine Learning problem. I am going to Implement a Machine Learning Algorithm to predict the median House Value of houses in California State.\n\nThe dataset that I will be using to train and build model is derived from 1990 California Census.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nEXPLORATORY DATA ANALYSIS:\n\nIn this step we will try to understand our dataset better by exploring its structure and contents.This will help us learn about  relationships and patterns among its variables.\n","metadata":{"papermill":{"duration":0.021876,"end_time":"2022-01-28T09:01:31.570777","exception":false,"start_time":"2022-01-28T09:01:31.548901","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nPART 1: \n    Importing Python Libraries for the analysis\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"papermill":{"duration":1.118549,"end_time":"2022-01-28T09:01:32.712626","exception":false,"start_time":"2022-01-28T09:01:31.594077","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.268124Z","iopub.execute_input":"2022-07-07T07:26:31.268422Z","iopub.status.idle":"2022-07-07T07:26:31.275554Z","shell.execute_reply.started":"2022-07-07T07:26:31.268396Z","shell.execute_reply":"2022-07-07T07:26:31.274418Z"},"trusted":true},"execution_count":756,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPART 2: Loading the dataset\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/california-housing-prices/housing.csv')\n","metadata":{"papermill":{"duration":0.109396,"end_time":"2022-01-28T09:01:32.842731","exception":false,"start_time":"2022-01-28T09:01:32.733335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.277131Z","iopub.execute_input":"2022-07-07T07:26:31.277828Z","iopub.status.idle":"2022-07-07T07:26:31.319064Z","shell.execute_reply.started":"2022-07-07T07:26:31.277783Z","shell.execute_reply":"2022-07-07T07:26:31.318060Z"},"trusted":true},"execution_count":757,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n Splitting Our Dataset into Training and Testing.","metadata":{}},{"cell_type":"code","source":"#Splitting the dataset into train and test to avoid overfitting\ntest = df.iloc[:,-2]\ntrain = df.drop(['median_house_value'],axis =1)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.320434Z","iopub.execute_input":"2022-07-07T07:26:31.320745Z","iopub.status.idle":"2022-07-07T07:26:31.327305Z","shell.execute_reply.started":"2022-07-07T07:26:31.320706Z","shell.execute_reply":"2022-07-07T07:26:31.326199Z"},"trusted":true},"execution_count":758,"outputs":[]},{"cell_type":"code","source":"#Splitting training, testing data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train, test , test_size = .25, random_state = 2)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.335400Z","iopub.execute_input":"2022-07-07T07:26:31.336048Z","iopub.status.idle":"2022-07-07T07:26:31.345486Z","shell.execute_reply.started":"2022-07-07T07:26:31.336011Z","shell.execute_reply":"2022-07-07T07:26:31.344252Z"},"trusted":true},"execution_count":759,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nPART 3: Structure Based Exploratoty Data Analysis\n    \n    \nHere we are going to understand the metadata; the description of the data we have our in our dataframe\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\ni. Display the first 5 Observations\n","metadata":{}},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.346815Z","iopub.execute_input":"2022-07-07T07:26:31.347161Z","iopub.status.idle":"2022-07-07T07:26:31.371065Z","shell.execute_reply.started":"2022-07-07T07:26:31.347130Z","shell.execute_reply":"2022-07-07T07:26:31.370060Z"},"trusted":true},"execution_count":760,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nii. Display the last 5 Observations\n","metadata":{}},{"cell_type":"code","source":"x_train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.372866Z","iopub.execute_input":"2022-07-07T07:26:31.373263Z","iopub.status.idle":"2022-07-07T07:26:31.395105Z","shell.execute_reply.started":"2022-07-07T07:26:31.373232Z","shell.execute_reply":"2022-07-07T07:26:31.393821Z"},"trusted":true},"execution_count":761,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\niii. Display the number of Variables and Number of Observation\n","metadata":{}},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.396521Z","iopub.execute_input":"2022-07-07T07:26:31.396850Z","iopub.status.idle":"2022-07-07T07:26:31.402384Z","shell.execute_reply.started":"2022-07-07T07:26:31.396820Z","shell.execute_reply":"2022-07-07T07:26:31.401300Z"},"trusted":true},"execution_count":762,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"font-size: 15px\">\n<font color='blue'>Our dataset has 10 Variables and 20,640 Observations.</font>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'>\n1. longitude: A measure of how far west a house is; a higher value is farther west\n\n2. latitude: A measure of how far north a house is; a higher value is farther north\n\n3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n\n4. totalRooms: Total number of rooms within a block\n\n5. totalBedrooms: Total number of bedrooms within a block\n\n6. population: Total number of people residing within a block\n\n7. households: Total number of households, a group of people residing within a home unit, for a block\n\n8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n\n9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n\n10. oceanProximity: Location of the house w.r.t ocean/sea\n   </font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\niv. Display the variables names and Datatypes\n","metadata":{}},{"cell_type":"code","source":"x_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.404064Z","iopub.execute_input":"2022-07-07T07:26:31.404395Z","iopub.status.idle":"2022-07-07T07:26:31.416107Z","shell.execute_reply.started":"2022-07-07T07:26:31.404366Z","shell.execute_reply":"2022-07-07T07:26:31.415085Z"},"trusted":true},"execution_count":763,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nOur dataset is comprised of 9 Numerical Variable and one Categorical Variable.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nv. Descriptive Statistics:\n    \nNow to know about the characteristics of the data set we will use the df.describe() method which by default gives the summary of all the numerical variables present in our data frame.\n\n","metadata":{}},{"cell_type":"code","source":"x_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.417607Z","iopub.execute_input":"2022-07-07T07:26:31.418819Z","iopub.status.idle":"2022-07-07T07:26:31.459514Z","shell.execute_reply.started":"2022-07-07T07:26:31.418774Z","shell.execute_reply":"2022-07-07T07:26:31.458164Z"},"trusted":true},"execution_count":764,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nvi.\nDisplay the Complete Meta-Data of the DataSet\n","metadata":{}},{"cell_type":"code","source":"#Summary of the DataFrame\nx_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.461845Z","iopub.execute_input":"2022-07-07T07:26:31.462294Z","iopub.status.idle":"2022-07-07T07:26:31.481070Z","shell.execute_reply.started":"2022-07-07T07:26:31.462251Z","shell.execute_reply":"2022-07-07T07:26:31.479796Z"},"trusted":true},"execution_count":765,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nPART 4: Content Based Exploratoty Data Analysis\n    \n    \nHere we are going to check at various contents of the dataset and fix the ones with any problem. \n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\n1.\nHandling Missing Data\n\n Here we are going to check for and handle the missing data in our dataset so that they don't affect our model performance.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\na. Checking for any missing values in our Dataset","metadata":{}},{"cell_type":"code","source":"#Checking for the missing values in the dataset\nx_train.isnull().sum()","metadata":{"papermill":{"duration":0.039952,"end_time":"2022-01-28T09:01:32.906458","exception":false,"start_time":"2022-01-28T09:01:32.866506","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.482862Z","iopub.execute_input":"2022-07-07T07:26:31.483384Z","iopub.status.idle":"2022-07-07T07:26:31.494136Z","shell.execute_reply.started":"2022-07-07T07:26:31.483339Z","shell.execute_reply":"2022-07-07T07:26:31.493039Z"},"trusted":true},"execution_count":766,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: It is only 'total_bedrooms' variable that has missing values totalling 207 observations</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nb. Fixing the Dataset Problem by filling the missing values rows with the mean of the 'total_bedrooms' variable(Missing Value Imputation Technique)\n    \nHere we are going to implement the fillna() python function","metadata":{}},{"cell_type":"code","source":"\nx_train['total_bedrooms']=x_train['total_bedrooms'].fillna(x_train['total_bedrooms'].mean())\n","metadata":{"papermill":{"duration":0.052469,"end_time":"2022-01-28T09:01:32.984335","exception":false,"start_time":"2022-01-28T09:01:32.931866","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.495403Z","iopub.execute_input":"2022-07-07T07:26:31.496374Z","iopub.status.idle":"2022-07-07T07:26:31.505301Z","shell.execute_reply.started":"2022-07-07T07:26:31.496340Z","shell.execute_reply":"2022-07-07T07:26:31.504311Z"},"trusted":true},"execution_count":767,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nc. Checking for any missing values in updated Dataset","metadata":{}},{"cell_type":"code","source":"x_train.isnull().sum()","metadata":{"papermill":{"duration":0.039099,"end_time":"2022-01-28T09:01:33.045016","exception":false,"start_time":"2022-01-28T09:01:33.005917","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.506838Z","iopub.execute_input":"2022-07-07T07:26:31.507447Z","iopub.status.idle":"2022-07-07T07:26:31.524508Z","shell.execute_reply.started":"2022-07-07T07:26:31.507417Z","shell.execute_reply":"2022-07-07T07:26:31.523520Z"},"trusted":true},"execution_count":768,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: No variable has a missing value.\nThat means we have fixed the missing value problem in the dataset and we are ready to move on</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\n2. Handling Duplicate Values:\n Here we are going to check for Observations repeating themselves and if we find any we will drop them off from our dataset.\n\n​","metadata":{}},{"cell_type":"code","source":"#Check for duplicated rows\nx_train.duplicated().sum()","metadata":{"papermill":{"duration":0.044574,"end_time":"2022-01-28T09:01:33.112538","exception":false,"start_time":"2022-01-28T09:01:33.067964","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:26:31.525898Z","iopub.execute_input":"2022-07-07T07:26:31.526443Z","iopub.status.idle":"2022-07-07T07:26:31.541954Z","shell.execute_reply.started":"2022-07-07T07:26:31.526412Z","shell.execute_reply":"2022-07-07T07:26:31.540800Z"},"trusted":true},"execution_count":769,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: No duplicate Observations in our dataset.</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: Using the df.describe() method we get the following characteristics of the numerical variables namely to count (number of non-missing values), mean, standard deviation, and the 5 point summary which includes minimum, first quartile, second quartile, third quartile, and maximum..</font>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\n3. Handling Outliers:\n Here we are going to visualiaze our dataset to see if there are any outliers using the box plot method.\n If the results from our visualization proves that we have outliers then we are going to take a step further to treat them appropriately.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\na. Firstly, I am going to separate the Numerical Variables and Categorical Variables. \n\n","metadata":{}},{"cell_type":"code","source":"#Separating Categorical Features from Numerical Features\ncategoricalVariables = x_train.select_dtypes(include=['object'])\nnumericalVariables = x_train.select_dtypes(include = ['int32', 'int64', 'float32', 'float64'])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.543253Z","iopub.execute_input":"2022-07-07T07:26:31.544342Z","iopub.status.idle":"2022-07-07T07:26:31.554003Z","shell.execute_reply.started":"2022-07-07T07:26:31.544296Z","shell.execute_reply":"2022-07-07T07:26:31.552685Z"},"trusted":true},"execution_count":770,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nb. Secondly, I am going to put both the Variables Categories into their repsective python lists.\n ","metadata":{}},{"cell_type":"code","source":"numericalVariablesList= list(numericalVariables)\ncategoricalVarialeList= list(categoricalVariables)\nprint(numericalVariables, categoricalVariables)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.555386Z","iopub.execute_input":"2022-07-07T07:26:31.556002Z","iopub.status.idle":"2022-07-07T07:26:31.576557Z","shell.execute_reply.started":"2022-07-07T07:26:31.555968Z","shell.execute_reply":"2022-07-07T07:26:31.575720Z"},"trusted":true},"execution_count":771,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: Both the categorical and numerical variables have been put in their respective lists .</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nc. Thirdly, Visualizing the Outliers using box plot\n\n","metadata":{}},{"cell_type":"code","source":"sns.set_style('dark')\nfor column in numericalVariablesList:\n    plt.figure(figsize=(12,5))\n    #the box plot\n    plt.subplot(132)\n    sns.boxplot(x=df[column])\n    \n    #the distribution plot\n    plt.subplot(131)\n    sns.histplot(x=df[column], label=\"skew: \" + str(np.round(df[column].skew(),2)))\n    plt.legend()\n  \n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:31.577940Z","iopub.execute_input":"2022-07-07T07:26:31.578410Z","iopub.status.idle":"2022-07-07T07:26:41.554653Z","shell.execute_reply.started":"2022-07-07T07:26:31.578380Z","shell.execute_reply":"2022-07-07T07:26:41.553548Z"},"trusted":true},"execution_count":772,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nc. Fourthly, Treating Outliers:\n Here we are going to handle and remove the outliers from our dataset.\n </font>\n​","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\n  \nUSING QUANTILE BASED FLOORING TECHNIQUE TO TREAT OUTLIERS\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\n  \nQuantile based flooring and capping:\n    \nIn this technique, the outlier is capped at a certain value above the 90th percentile value or floored at a factor below the 10th percentile value.\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nCreating the iqr_capping function.\n ","metadata":{}},{"cell_type":"code","source":"def iqr_capping(cols, factor):\n    #df - dataset\n    #cols - list of numerical columns\n    for col in cols:\n        \n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        \n        iqr = q3 - q1\n        \n        upper_whisker = q3 + (factor*iqr)\n        lower_whisker = q1 - (factor*iqr)\n        \n        df[col] = np.where(df[col]>upper_whisker, upper_whisker,\n                 np.where(df[col]<lower_whisker, lower_whisker, df[col]))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:41.556078Z","iopub.execute_input":"2022-07-07T07:26:41.556397Z","iopub.status.idle":"2022-07-07T07:26:41.563661Z","shell.execute_reply.started":"2022-07-07T07:26:41.556365Z","shell.execute_reply":"2022-07-07T07:26:41.562405Z"},"trusted":true},"execution_count":773,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nPassing the list of columns arguments and factor to the iqr_capping function.\n\n","metadata":{}},{"cell_type":"code","source":"iqr_capping(numericalVariablesList, 1.5)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:41.566110Z","iopub.execute_input":"2022-07-07T07:26:41.566475Z","iopub.status.idle":"2022-07-07T07:26:41.598319Z","shell.execute_reply.started":"2022-07-07T07:26:41.566442Z","shell.execute_reply":"2022-07-07T07:26:41.597174Z"},"trusted":true},"execution_count":774,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n\nVisualizing Outliers using box plots after capping them to compare to the earlier Visualization of outliers before capping.\n\n","metadata":{}},{"cell_type":"code","source":"sns.set_style('dark')\nfor column in numericalVariablesList:\n    plt.figure(figsize=(12,5))\n    \n    #the box plot\n    plt.subplot(132)\n    sns.boxplot(x=df[column])\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:41.599686Z","iopub.execute_input":"2022-07-07T07:26:41.600066Z","iopub.status.idle":"2022-07-07T07:26:44.951227Z","shell.execute_reply.started":"2022-07-07T07:26:41.600036Z","shell.execute_reply":"2022-07-07T07:26:44.950035Z"},"trusted":true},"execution_count":775,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: All the outliers that we had seen earlier in our variables have been treated well. We are now ready to move to our next  preprocessing step</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPART 5: MULTIVARIATE DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n Numerical Variables","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n1. Plotting the Pairplot for the numerical DataFrames:\n    \nI am using Pairplot in this case so that I may easily be able to visualize and analyze the variables and the relationships between them.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(numericalVariables,height=3,size=2.5)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:26:44.952442Z","iopub.execute_input":"2022-07-07T07:26:44.952750Z","iopub.status.idle":"2022-07-07T07:27:12.713650Z","shell.execute_reply.started":"2022-07-07T07:26:44.952709Z","shell.execute_reply":"2022-07-07T07:27:12.712488Z"},"trusted":true},"execution_count":776,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPART 6: FEATURE SELECTION:\n\nHere I am going to be selecting a subset of relevant features to use in our model construction.We are going to use various Techniques to achieve it.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n We will do a little Data Prepocessing before implementing our Feature Selection Techniques","metadata":{}},{"cell_type":"code","source":"#Label Encoding\n#I am going to use the Labelencoder() function from sklerarn to convert the Categorical Variables to numerical\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx_train['ocean_proximity'] = le.fit_transform(x_train['ocean_proximity'])\nx_test['ocean_proximity'] = le.fit_transform(x_test['ocean_proximity'])","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:12.715385Z","iopub.execute_input":"2022-07-07T07:27:12.716083Z","iopub.status.idle":"2022-07-07T07:27:12.726930Z","shell.execute_reply.started":"2022-07-07T07:27:12.716048Z","shell.execute_reply":"2022-07-07T07:27:12.726022Z"},"trusted":true},"execution_count":777,"outputs":[]},{"cell_type":"code","source":"x_train['ocean_proximity'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:12.728276Z","iopub.execute_input":"2022-07-07T07:27:12.729709Z","iopub.status.idle":"2022-07-07T07:27:12.742998Z","shell.execute_reply.started":"2022-07-07T07:27:12.729653Z","shell.execute_reply":"2022-07-07T07:27:12.741581Z"},"trusted":true},"execution_count":778,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:12.745131Z","iopub.execute_input":"2022-07-07T07:27:12.746293Z","iopub.status.idle":"2022-07-07T07:27:12.767124Z","shell.execute_reply.started":"2022-07-07T07:27:12.746245Z","shell.execute_reply":"2022-07-07T07:27:12.765985Z"},"trusted":true},"execution_count":779,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nTechnique 1: Feature Selection with Variance Threshold\n    \nFeature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the desired outputs (y)","metadata":{}},{"cell_type":"code","source":"#It will check for zero variance in features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres= VarianceThreshold(threshold=0)\n'''Threshold - Features with a training-set variance lower than this threshold '0' will be removed. The default is to keep \nall features with non-zero variance, i.e. remove the features that have the same value in all samples'''\nvar_thres.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:12.768313Z","iopub.execute_input":"2022-07-07T07:27:12.768622Z","iopub.status.idle":"2022-07-07T07:27:12.779894Z","shell.execute_reply.started":"2022-07-07T07:27:12.768593Z","shell.execute_reply":"2022-07-07T07:27:12.779102Z"},"trusted":true},"execution_count":780,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n I am going to use the .get_support() function to check for features with non-zero variance and I will drop any with less that 0 variance.","metadata":{}},{"cell_type":"code","source":"var_thres.get_support()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:12.781340Z","iopub.execute_input":"2022-07-07T07:27:12.781930Z","iopub.status.idle":"2022-07-07T07:27:12.788656Z","shell.execute_reply.started":"2022-07-07T07:27:12.781899Z","shell.execute_reply":"2022-07-07T07:27:12.787803Z"},"trusted":true},"execution_count":781,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 15px\">\n<font color='blue'> Results: All our Features have non-zero Variance. This means we will keep all of our features and move on to the next Feature selection technique.</font>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nTechnique 2: Feature Selection With Correlation\n    \n    \nIn this step we will be removing the features which are highly correlated","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPlotting the Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"#Correlation heatmap\nsns.set_style('darkgrid')\nplt.figure(figsize = None)\nsns.heatmap(x_train.corr(), annot= True, cmap= 'Dark2')\nplt.show()","metadata":{"papermill":{"duration":0.725597,"end_time":"2022-01-28T09:01:34.078633","exception":false,"start_time":"2022-01-28T09:01:33.353036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:27:12.790205Z","iopub.execute_input":"2022-07-07T07:27:12.791053Z","iopub.status.idle":"2022-07-07T07:27:13.567071Z","shell.execute_reply.started":"2022-07-07T07:27:12.790995Z","shell.execute_reply":"2022-07-07T07:27:13.565829Z"},"trusted":true},"execution_count":782,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n<font color='blue'> DISCUSSION OF THE HEATMAP RESULTS:\n 1. Varibales With Positive Correlation:\n    \n    Strong Positive Correlation:\n    i. House_holds|Total_rooms\n    ii. House_holds|Total_bedrooms\n    iii. House_holds|Population\n    iii. Total_bedrooms|Total_rooms\n    \n    High Positive Correlation:\n    i. Population|Total_rooms\n    ii. Population|Total_bedrooms\n    \n    Average Positive Correlation:\n    i. Median_house_value|Median_income\n    \n 2. Varibales With Negative Correlation:\n    \n    Low Negative Correlation:\n    i. House_median_age|Total_rooms\n    ii. House_median_age|Total_bedrooms\n    iii. House_median_age|Population\n    iii. House_median_age|Population\n    \n    ","metadata":{}},{"cell_type":"code","source":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:13.568819Z","iopub.execute_input":"2022-07-07T07:27:13.569516Z","iopub.status.idle":"2022-07-07T07:27:13.577015Z","shell.execute_reply.started":"2022-07-07T07:27:13.569477Z","shell.execute_reply":"2022-07-07T07:27:13.575638Z"},"trusted":true},"execution_count":783,"outputs":[]},{"cell_type":"code","source":"#Checking for highly correlated features (greater than correlation of > +-0.9)\ncorr_features = correlation(x_train, 0.9)\nlen(set(corr_features))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:13.578665Z","iopub.execute_input":"2022-07-07T07:27:13.579149Z","iopub.status.idle":"2022-07-07T07:27:13.597595Z","shell.execute_reply.started":"2022-07-07T07:27:13.579106Z","shell.execute_reply":"2022-07-07T07:27:13.596773Z"},"trusted":true},"execution_count":784,"outputs":[]},{"cell_type":"code","source":"#Printing out the 3 features that are highly correlated with the other Features\ncorr_features","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:13.599323Z","iopub.execute_input":"2022-07-07T07:27:13.600149Z","iopub.status.idle":"2022-07-07T07:27:13.607288Z","shell.execute_reply.started":"2022-07-07T07:27:13.600105Z","shell.execute_reply":"2022-07-07T07:27:13.606473Z"},"trusted":true},"execution_count":785,"outputs":[]},{"cell_type":"code","source":"#Dropping the highly correlated features to reduce the duplicate features in our model\nx_train = x_train.drop(corr_features,axis=1)\nx_test= x_test.drop(corr_features,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:13.608756Z","iopub.execute_input":"2022-07-07T07:27:13.609473Z","iopub.status.idle":"2022-07-07T07:27:13.620408Z","shell.execute_reply.started":"2022-07-07T07:27:13.609415Z","shell.execute_reply":"2022-07-07T07:27:13.619348Z"},"trusted":true},"execution_count":786,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nTechnique 3: Feature Selection With Mutual Information Regressor\n    \nMutual Information\nEstimate mutual information for a continuous target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n    ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n# determine the mutual information\nmutual_info = mutual_info_regression(x_train, y_train)\nmutual_info","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:13.622034Z","iopub.execute_input":"2022-07-07T07:27:13.622787Z","iopub.status.idle":"2022-07-07T07:27:14.220843Z","shell.execute_reply.started":"2022-07-07T07:27:13.622716Z","shell.execute_reply":"2022-07-07T07:27:14.219532Z"},"trusted":true},"execution_count":787,"outputs":[]},{"cell_type":"code","source":"#Printing out the most important Features in the Descending order\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = x_train.columns\nmutual_info.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:14.222188Z","iopub.execute_input":"2022-07-07T07:27:14.222513Z","iopub.status.idle":"2022-07-07T07:27:14.231623Z","shell.execute_reply.started":"2022-07-07T07:27:14.222483Z","shell.execute_reply":"2022-07-07T07:27:14.230579Z"},"trusted":true},"execution_count":788,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectPercentile\n\n# Selecting the top 100 percentile from the List of important Features Above\n'''I am picking the top 100 percentile (All my Features). This is because I have small number of features,\nif I had a large dataset with many columns I would have easily used 20, 30, 40, or 50 Percentile depending \non how big the dataset is'''\n\nselected_top_columns = SelectPercentile(mutual_info_regression, percentile=100)\nselected_top_columns.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:14.233040Z","iopub.execute_input":"2022-07-07T07:27:14.233588Z","iopub.status.idle":"2022-07-07T07:27:14.814600Z","shell.execute_reply.started":"2022-07-07T07:27:14.233553Z","shell.execute_reply":"2022-07-07T07:27:14.813580Z"},"trusted":true},"execution_count":789,"outputs":[]},{"cell_type":"code","source":"selected_top_columns.get_support()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:14.815890Z","iopub.execute_input":"2022-07-07T07:27:14.816220Z","iopub.status.idle":"2022-07-07T07:27:14.823298Z","shell.execute_reply.started":"2022-07-07T07:27:14.816192Z","shell.execute_reply":"2022-07-07T07:27:14.822208Z"},"trusted":true},"execution_count":790,"outputs":[]},{"cell_type":"code","source":"#Priting out our final subset of Features \n\nx_train.columns[selected_top_columns.get_support()]","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:14.824935Z","iopub.execute_input":"2022-07-07T07:27:14.825597Z","iopub.status.idle":"2022-07-07T07:27:14.836276Z","shell.execute_reply.started":"2022-07-07T07:27:14.825555Z","shell.execute_reply":"2022-07-07T07:27:14.835144Z"},"trusted":true},"execution_count":791,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"font-size: 20px\">\n<font color='blue'> DISCUSSION OF THE HEATMAP RESULTS:Results Discussion:\n    \nFrom the Results above,we have the final set of Features we need to build the Machine Learning Model.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPART 6: MODEL BUILDING:\n","metadata":{"papermill":{"duration":0.064621,"end_time":"2022-01-28T09:01:45.152318","exception":false,"start_time":"2022-01-28T09:01:45.087697","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Import the necessary Libraries for Model Building\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xg\n\n\n","metadata":{"papermill":{"duration":0.350193,"end_time":"2022-01-28T09:01:45.563242","exception":false,"start_time":"2022-01-28T09:01:45.213049","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:27:14.837582Z","iopub.execute_input":"2022-07-07T07:27:14.838234Z","iopub.status.idle":"2022-07-07T07:27:14.846295Z","shell.execute_reply.started":"2022-07-07T07:27:14.838150Z","shell.execute_reply":"2022-07-07T07:27:14.845206Z"},"trusted":true},"execution_count":792,"outputs":[]},{"cell_type":"code","source":"#Standardize Data (Feature Scaling)\n#feature scaling is a method used to normalize the range of independent variables or features of data.\n\nss = StandardScaler()\ncolumn_names = x_train.columns\nx_train = pd.DataFrame(ss.fit_transform(x_train), columns =column_names)\nx_test = pd.DataFrame(ss.fit_transform(x_test), columns =column_names)\n","metadata":{"papermill":{"duration":0.085398,"end_time":"2022-01-28T09:01:47.321042","exception":false,"start_time":"2022-01-28T09:01:47.235644","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-07-07T07:27:14.847826Z","iopub.execute_input":"2022-07-07T07:27:14.848430Z","iopub.status.idle":"2022-07-07T07:27:14.867522Z","shell.execute_reply.started":"2022-07-07T07:27:14.848397Z","shell.execute_reply":"2022-07-07T07:27:14.866219Z"},"trusted":true},"execution_count":793,"outputs":[]},{"cell_type":"code","source":"#Machine Learning Algorithms\n#Instatiating the models\nlinearRegression = LinearRegression()\nxgb_r = xg.XGBRegressor() \nrandomForest=RandomForestRegressor()\n\nMachineLearningAlgorithms= [linearRegression, xgb_r, randomForest]\nMachineLearningAlgorithms\n\nfor MachineLearningAlgorithm in MachineLearningAlgorithms:\n    \n    #fitting the model\n    print(f'fitting the {MachineLearningAlgorithm} model')\n    print()\n    MachineLearningAlgorithm= MachineLearningAlgorithm.fit(x_train,y_train)\n    \n    #Training Data Prediction and Metrics\n    #Predictions\n    y_pred = MachineLearningAlgorithm.predict(x_train)\n    #Metrics\n    print(f'The Metrics of the {MachineLearningAlgorithm} Training model')\n    print('R2 score',metrics.r2_score(y_train,y_pred))\n    #print('Mean Absolute Error',metrics.mean_absolute_error(y_train,y_pred))\n    #print('Root Mean Square Error',metrics.mean_squared_error(y_train,y_pred,squared=False))\n    \n    print()\n    \n    #Testing Data Prediction and Metrics\n    #Predictions\n    y_pred = MachineLearningAlgorithm.predict(x_test)\n    #Metrics\n    print(f'The Metrics of the {MachineLearningAlgorithm} Testing model')\n    print('R2 score',metrics.r2_score(y_test,y_pred))\n    #print('Mean Absolute Error',metrics.mean_absolute_error(y_train,y_pred))\n    #print('Root Mean Square Error',metrics.mean_squared_error(y_train,y_pred,squared=False))\n    \n    print()","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:14.870633Z","iopub.execute_input":"2022-07-07T07:27:14.871483Z","iopub.status.idle":"2022-07-07T07:27:24.108247Z","shell.execute_reply.started":"2022-07-07T07:27:14.871446Z","shell.execute_reply":"2022-07-07T07:27:24.107119Z"},"trusted":true},"execution_count":794,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\n<font color='blue'> DISCUSSION OF RESULTS:\n \nI have used three different machine learning algorithm to build my model(linear regression, xgboost regressor, randomforest regressor. From the Performance accuracy, It is evident that the xgboost model had the highest accuracy than the other two. I am going to save the xgboost model for deploying because it has proven to be the best.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size: 20px\">\nPART 7: SAVING OUR MODEL USING PICKLE PACKAGE\n    \nPickle in Python is primarily used in serializing and deserializing a Python object structure. In other words, it's the process of converting a Python object into a byte stream to store it in a file/database.","metadata":{}},{"cell_type":"code","source":"#import our pickle package\nimport pickle\n#Saving the model\npickle.dump(xgb_r, open('xgb_model.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T07:27:24.109825Z","iopub.execute_input":"2022-07-07T07:27:24.110146Z","iopub.status.idle":"2022-07-07T07:27:24.123137Z","shell.execute_reply.started":"2022-07-07T07:27:24.110117Z","shell.execute_reply":"2022-07-07T07:27:24.122189Z"},"trusted":true},"execution_count":795,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}